{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/haoran/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.layers import  Input, Dense, Flatten, Add, LSTM, GlobalAveragePooling1D,SpatialDropout1D, Bidirectional,\\\n",
    "    BatchNormalization, Concatenate, Dropout, Activation, Input, Embedding, Conv1D, MaxPooling1D, GRU,\\\n",
    "    GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "import tensorflow as tf\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard, Callback\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "class ROCCallBack(Callback):\n",
    "    def __init__(self,validation_data):\n",
    "        super().__init__()\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        print('\\nroc-auc_val: %s' % (str(round(roc_auc_score(self.y_val, y_pred_val),4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    if isinstance(x, (np.ndarray, list, tuple, pd.Series)):\n",
    "        lst = []\n",
    "        for i in x:\n",
    "            lst += flatten(i)\n",
    "        return lst\n",
    "    else:\n",
    "        return [x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_names = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok=text.Tokenizer(filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_\\'`{|}~\\t\\n', lower=True)\n",
    "tok.fit_on_texts(np.concatenate((train.comment_text.values, test.comment_text.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('data/glove.42B.300d.txt', 'r', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_unique_tokens = tok.word_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    # Whole GloVe embeddings doesn't fit in my GPU memory, so only take words which appear in data for now. \n",
    "    # Can always swap weights for embedding layer after model training\n",
    "    if word in all_unique_tokens:\n",
    "        coefs = np.array(values[1:], dtype = 'float32')\n",
    "        embeddings[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in list(tok.word_index.keys()):\n",
    "    if i not in embeddings.keys():\n",
    "        del tok.word_index[i]\n",
    "for counter, i in enumerate(tok.word_index.keys()):\n",
    "    tok.word_index[i] = counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx2word = {b:a for a,b in tok.word_index.items()}\n",
    "idx2word[0] = '<UNK>'\n",
    "word2idx = defaultdict(lambda x: '<UNK>', tok.word_index)\n",
    "embeddings['<UNK>'] = np.zeros((300,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['toks'] = tok.texts_to_sequences(train.comment_text.values)\n",
    "test['toks'] = tok.texts_to_sequences(test.comment_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(embeddings)\n",
    "max_len = 300\n",
    "n_factors = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_emb():\n",
    "    emb = np.zeros((vocab_size+1,n_factors), dtype = 'float32')\n",
    "    for i in range(0, vocab_size):\n",
    "        word = idx2word[i]\n",
    "        emb[i,:] = embeddings[word] #each row is a word\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190324, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train val  split\n",
    "np.random.seed(10)\n",
    "indexTrain = np.random.choice(range(train.shape[0]), size = int(0.9*train.shape[0]), replace = False)\n",
    "indexVal = list(set(range(train.shape[0])) - set(indexTrain))\n",
    "traindf = train.loc[indexTrain]\n",
    "valdf = train.loc[indexVal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataInputTrain=sequence.pad_sequences(traindf.toks,maxlen=max_len)\n",
    "dataInputVal=sequence.pad_sequences(valdf.toks,maxlen=max_len)\n",
    "dataInputTest=sequence.pad_sequences(test.toks,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> reviewing edits hi it appears you approved this edit which was the change of a cited number to a different number without changing the citation that edit is not an improvement to the article as it adds false or misleading information to the text it cites a source to a number where the source does not support that number and it should not have been approved please be careful when approving these types of edits best'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[i] for i in dataInputTrain[10,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeModel(counter, denseNodes, convFilters, dropOut):\n",
    "    sequence_input = Input(shape=(max_len, ))\n",
    "    x = Embedding(vocab_size+1, n_factors, input_length=max_len, weights=[emb],trainable = False)(sequence_input)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True,dropout=0.15,recurrent_dropout=0.15))(x)\n",
    "    x = Conv1D(convFilters, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x = Concatenate()([avg_pool, max_pool])\n",
    "    x = Dense(denseNodes, activation = 'relu')(x)\n",
    "    x = BatchNormalization(axis = -1)(x)\n",
    "    x = Dropout(dropOut)(x)\n",
    "    preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3))\n",
    "    \n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min')\n",
    "    mcp_save = ModelCheckpoint('weights/lstm_mdl' + str(counter), save_best_only=True, monitor='val_loss', mode='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n",
    "    roc_callback = ROCCallBack(validation_data = [dataInputVal, valdf[output_names].values])\n",
    "    \n",
    "    model.fit(x = dataInputTrain,\n",
    "         y = traindf[output_names].values,\n",
    "         batch_size = 64, epochs = 200,\n",
    "         validation_data = [dataInputVal, valdf[output_names].values],\n",
    "         callbacks=[earlyStopping, mcp_save, reduce_lr_loss, roc_callback])\n",
    "    \n",
    "    pred = model.predict(dataInputTest, verbose = 1)\n",
    "    for counter,i in enumerate(output_names):\n",
    "        test[i] = pred[:,counter]\n",
    "    test[['id'] + output_names].to_csv('data/answers/lstm' + str(counter) + '.csv', index = False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = [\n",
    "    {'denseNodes': 128, 'convFilters': 128, 'dropOut': 0.4},\n",
    "    {'denseNodes': 256, 'convFilters': 256, 'dropOut': 0.5},\n",
    "    {'denseNodes': 256, 'convFilters': 128, 'dropOut': 0.5},\n",
    "]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0870\n",
      "roc-auc_val: 0.9832\n",
      "143613/143613 [==============================] - 1352s 9ms/step - loss: 0.0870 - val_loss: 0.0464\n",
      "Epoch 2/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0488\n",
      "roc-auc_val: 0.9879\n",
      "143613/143613 [==============================] - 1340s 9ms/step - loss: 0.0488 - val_loss: 0.0439\n",
      "Epoch 3/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0448\n",
      "roc-auc_val: 0.9889\n",
      "143613/143613 [==============================] - 1341s 9ms/step - loss: 0.0448 - val_loss: 0.0420\n",
      "Epoch 4/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0421\n",
      "roc-auc_val: 0.989\n",
      "143613/143613 [==============================] - 1340s 9ms/step - loss: 0.0421 - val_loss: 0.0419\n",
      "Epoch 5/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0395\n",
      "roc-auc_val: 0.989\n",
      "143613/143613 [==============================] - 1340s 9ms/step - loss: 0.0395 - val_loss: 0.0419\n",
      "Epoch 6/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0371\n",
      "roc-auc_val: 0.9887\n",
      "143613/143613 [==============================] - 1340s 9ms/step - loss: 0.0371 - val_loss: 0.0422\n",
      "Epoch 7/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0349\n",
      "Epoch 00007: reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "roc-auc_val: 0.9887\n",
      "143613/143613 [==============================] - 1340s 9ms/step - loss: 0.0349 - val_loss: 0.0429\n",
      "Epoch 8/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0297\n",
      "roc-auc_val: 0.9879\n",
      "143613/143613 [==============================] - 1340s 9ms/step - loss: 0.0297 - val_loss: 0.0428\n",
      "Epoch 9/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0280\n",
      "roc-auc_val: 0.9872\n",
      "143613/143613 [==============================] - 1339s 9ms/step - loss: 0.0280 - val_loss: 0.0442\n",
      "Epoch 10/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0270\n",
      "Epoch 00010: reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "roc-auc_val: 0.9867\n",
      "143613/143613 [==============================] - 1340s 9ms/step - loss: 0.0270 - val_loss: 0.0452\n",
      "153164/153164 [==============================] - 697s 5ms/step\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0802\n",
      "roc-auc_val: 0.9848\n",
      "143613/143613 [==============================] - 1400s 10ms/step - loss: 0.0802 - val_loss: 0.0490\n",
      "Epoch 2/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0513\n",
      "roc-auc_val: 0.987\n",
      "143613/143613 [==============================] - 1398s 10ms/step - loss: 0.0513 - val_loss: 0.0439\n",
      "Epoch 3/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0465\n",
      "roc-auc_val: 0.9879\n",
      "143613/143613 [==============================] - 1399s 10ms/step - loss: 0.0465 - val_loss: 0.0507\n",
      "Epoch 4/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0437\n",
      "roc-auc_val: 0.9888\n",
      "143613/143613 [==============================] - 1399s 10ms/step - loss: 0.0437 - val_loss: 0.0407\n",
      "Epoch 5/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0408\n",
      "roc-auc_val: 0.9893\n",
      "143613/143613 [==============================] - 1399s 10ms/step - loss: 0.0407 - val_loss: 0.0421\n",
      "Epoch 6/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0388\n",
      "roc-auc_val: 0.9888\n",
      "143613/143613 [==============================] - 1399s 10ms/step - loss: 0.0388 - val_loss: 0.0486\n",
      "Epoch 7/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0364\n",
      "roc-auc_val: 0.9888\n",
      "143613/143613 [==============================] - 1399s 10ms/step - loss: 0.0364 - val_loss: 0.0447\n",
      "Epoch 8/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0345\n",
      "Epoch 00008: reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "roc-auc_val: 0.9886\n",
      "143613/143613 [==============================] - 1398s 10ms/step - loss: 0.0345 - val_loss: 0.0413\n",
      "Epoch 9/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0299\n",
      "roc-auc_val: 0.9886\n",
      "143613/143613 [==============================] - 1399s 10ms/step - loss: 0.0299 - val_loss: 0.0425\n",
      "153164/153164 [==============================] - 705s 5ms/step\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0807\n",
      "roc-auc_val: 0.985\n",
      "143613/143613 [==============================] - 1341s 9ms/step - loss: 0.0807 - val_loss: 0.0557\n",
      "Epoch 2/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0498\n",
      "roc-auc_val: 0.9869\n",
      "143613/143613 [==============================] - 1340s 9ms/step - loss: 0.0498 - val_loss: 0.0424\n",
      "Epoch 3/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0455\n",
      "roc-auc_val: 0.9886\n",
      "143613/143613 [==============================] - 1340s 9ms/step - loss: 0.0455 - val_loss: 0.0410\n",
      "Epoch 4/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0427\n",
      "roc-auc_val: 0.9893\n",
      "143613/143613 [==============================] - 1339s 9ms/step - loss: 0.0428 - val_loss: 0.0424\n",
      "Epoch 5/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0404\n",
      "roc-auc_val: 0.9882\n",
      "143613/143613 [==============================] - 1339s 9ms/step - loss: 0.0404 - val_loss: 0.0413\n",
      "Epoch 6/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0381\n",
      "roc-auc_val: 0.9887\n",
      "143613/143613 [==============================] - 1339s 9ms/step - loss: 0.0380 - val_loss: 0.0424\n",
      "Epoch 7/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0353\n",
      "Epoch 00007: reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "roc-auc_val: 0.9883\n",
      "143613/143613 [==============================] - 1339s 9ms/step - loss: 0.0353 - val_loss: 0.0432\n",
      "Epoch 8/200\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0303\n",
      "roc-auc_val: 0.9881\n",
      "143613/143613 [==============================] - 1340s 9ms/step - loss: 0.0303 - val_loss: 0.0423\n",
      "153164/153164 [==============================] - 701s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "models = [makeModel(counter, **i) for counter, i in enumerate(params)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
