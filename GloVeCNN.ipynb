{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import  Input, Dense, Flatten, Add,\\\n",
    "    BatchNormalization, Concatenate, Dropout, Activation, Input, Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "import tensorflow as tf\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard, Callback\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# need to modify to work for 6 labels\n",
    "class ROCCallBack(Callback):\n",
    "    def __init__(self,training_data,validation_data):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.x)\n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        roc_train, roc_val = [], []\n",
    "        for counter, i in enumerate(output_names):            \n",
    "            roc_train.append(roc_auc_score(self.y[counter], y_pred[counter].flatten()))            \n",
    "            roc_val.append(roc_auc_score(self.y_val[counter], y_pred_val[counter].flatten())) \n",
    "        print()\n",
    "        print('roc-auc: %s - roc-auc_val: %s' % (str(round(np.mean(roc_train),4)),str(round(np.mean(roc_val),4))))\n",
    "        print('Val ROCs: ' + '    '.join([output_names[counter] + ': ' + str(round(i,4))for counter, i in enumerate(roc_val)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    if isinstance(x, (np.ndarray, list, tuple, pd.Series)):\n",
    "        lst = []\n",
    "        for i in x:\n",
    "            lst += flatten(i)\n",
    "        return lst\n",
    "    else:\n",
    "        return [x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use linear preds as pseudo-labeled data\n",
    "linear_preds = pd.read_csv('data/answers/linear_ensemble_0.85_09724.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo = test.merge(linear_preds, how = 'left', on='id').sample(int(train.shape[0]*0.3), random_state = 10).round()\n",
    "pseudo[output_names] = pseudo[output_names].astype(int)\n",
    "train = train.append(pseudo, ignore_index = True).sample(frac=1, random_state = 10).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>184379</td>\n",
       "      <td>205582</td>\n",
       "      <td>194779</td>\n",
       "      <td>206878</td>\n",
       "      <td>196491</td>\n",
       "      <td>205786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23063</td>\n",
       "      <td>1860</td>\n",
       "      <td>12663</td>\n",
       "      <td>564</td>\n",
       "      <td>10951</td>\n",
       "      <td>1656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0  184379        205582   194779  206878  196491         205786\n",
       "1   23063          1860    12663     564   10951           1656"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[output_names].apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_names = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokens'] = train['comment_text'].apply(lambda x: list(filter(lambda z: len(z), map(lambda y: y.lower(), nltk.word_tokenize(x)))))\n",
    "test['tokens'] = test['comment_text'].apply(lambda x: list(filter(lambda z: len(z), map(lambda y: y.lower(), nltk.word_tokenize(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [``, ==, more, help, with, a, specific, editor...\n",
       "1    [==, wtf, !, !, ==, i, hate, bill, gates, with...\n",
       "2    [most, certainly, ,, if, you, can, specify, yo...\n",
       "3    [==, will, i, be, given, proper, chance, and, ...\n",
       "4    [==, hi, ,, i, 'm, on, the, internet, using, a...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['tokLength'] = train['tokens'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    207442.000000\n",
       "mean         79.208584\n",
       "std         121.973466\n",
       "min           1.000000\n",
       "25%          20.000000\n",
       "50%          42.000000\n",
       "75%          87.000000\n",
       "max        4948.000000\n",
       "Name: tokLength, dtype: float64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tokLength'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc9362908d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['tokLength'].hist(bins = range(0, 1000, 20),linewidth = 1, edgecolor = 'black' )\n",
    "plt.show()\n",
    "#looks like max_length of 500 should be good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.235892442224809"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.percentileofscore(train['tokLength'].values, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_length = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most common words in the texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist = nltk.FreqDist(flatten(train.tokens.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total tokens in the training set: 16431187\n",
      "Number of unique tokens in the training set: 340323\n"
     ]
    }
   ],
   "source": [
    "print('Number of total tokens in the training set:', dist.N())\n",
    "print('Number of unique tokens in the training set:', dist.B())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 633575),\n",
       " ('the', 626620),\n",
       " (',', 598275),\n",
       " ('to', 372198),\n",
       " (\"''\", 321518),\n",
       " ('i', 294813),\n",
       " ('of', 282480),\n",
       " ('and', 280093),\n",
       " ('a', 273294),\n",
       " ('you', 270456),\n",
       " ('is', 230871),\n",
       " ('``', 202560),\n",
       " ('that', 202415),\n",
       " ('it', 188039),\n",
       " ('in', 182256),\n",
       " ('!', 166867),\n",
       " (':', 147354),\n",
       " ('for', 128326),\n",
       " ('this', 123310),\n",
       " ('not', 120640),\n",
       " (')', 112932),\n",
       " ('on', 111684),\n",
       " ('(', 105495),\n",
       " ('be', 105143),\n",
       " ('as', 96791),\n",
       " ('?', 94081),\n",
       " ('are', 92822),\n",
       " ('have', 91918),\n",
       " (\"'s\", 85430),\n",
       " ('your', 78390),\n",
       " ('do', 78187),\n",
       " ('with', 75141),\n",
       " ('if', 73187),\n",
       " (\"n't\", 72892),\n",
       " ('article', 71634),\n",
       " ('was', 71175),\n",
       " ('or', 66008),\n",
       " ('but', 64915),\n",
       " ('my', 55866),\n",
       " ('wikipedia', 55835),\n",
       " ('page', 55721),\n",
       " ('an', 55695),\n",
       " ('from', 51870),\n",
       " ('by', 51234),\n",
       " ('at', 49739),\n",
       " ('can', 46681),\n",
       " ('about', 46564),\n",
       " ('so', 45523),\n",
       " ('me', 45036),\n",
       " ('what', 44734)]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.most_common(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('data/glove.42B.300d.txt', 'r', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_unique_tokens = set(flatten(train.tokens.values)).union(set(flatten(test.tokens.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    # Whole GloVe embeddings doesn't fit in my GPU memory, so only take words which appear in data for now. \n",
    "    # Can always swap weights for embedding layer after model training\n",
    "    if word in all_unique_tokens:\n",
    "        coefs = np.array(values[1:], dtype = 'float32')\n",
    "        embeddings[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "embeddings['<UNK>'] = np.random.normal(scale = 0.6,size = embeddings['.'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most common words not in the vocab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notInVocab = []\n",
    "for i in dist.most_common():\n",
    "    if i[0] not in embeddings.keys() and i[1]>100:\n",
    "        notInVocab.append((i[0], i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('==', 26630),\n",
       " ('•', 6630),\n",
       " ('*', 5106),\n",
       " ('style=', 2441),\n",
       " ('|-', 2289),\n",
       " ('youfuck', 2078),\n",
       " ('~~~~', 1830),\n",
       " ('·', 1395),\n",
       " ('|style=', 1175),\n",
       " (':i', 1031),\n",
       " ('f5fffa', 1012),\n",
       " ('dixz', 1000),\n",
       " ('width=', 994),\n",
       " ('||', 994),\n",
       " ('..', 962),\n",
       " ('—preceding', 723),\n",
       " ('====', 696),\n",
       " ('border:1px', 675),\n",
       " ('yourselfgo', 621),\n",
       " ('cellpadding=', 587),\n",
       " ('//en.wikipedia.org/w/index.php', 581),\n",
       " ('و', 552),\n",
       " ('bitchbot', 499),\n",
       " ('//', 494),\n",
       " (\"'image\", 491),\n",
       " ('→', 463),\n",
       " ('philippineslong', 420),\n",
       " ('つ', 399),\n",
       " ('◕_◕', 399),\n",
       " ('oompapa', 384),\n",
       " ('*the', 383),\n",
       " ('|class=', 382),\n",
       " ('three-revert', 380),\n",
       " ('===', 376),\n",
       " ('jelaton', 375),\n",
       " ('mainpagebg', 367),\n",
       " ('در', 351),\n",
       " (\"sex'butt\", 350),\n",
       " ('pro-assad.hanibal911you', 345),\n",
       " ('\\u200e', 342),\n",
       " ('an/i', 339),\n",
       " (\"'fuck\", 334),\n",
       " ('bitches.fuck', 333),\n",
       " ('deneid', 331),\n",
       " ('rice=', 330),\n",
       " ('zeuphoris', 330),\n",
       " ('cellspacing=', 326),\n",
       " (\"'the\", 312),\n",
       " ('pagedelete', 312),\n",
       " ('notrhbysouthbanof', 308),\n",
       " ('cuntliz', 308),\n",
       " ('*how', 307),\n",
       " ('admin-', 289),\n",
       " ('criminalwar', 279),\n",
       " ('edit-warring', 278),\n",
       " ('bunksteve', 278),\n",
       " ('fuckfuck', 275),\n",
       " ('که', 271),\n",
       " ('//en.wikipedia.org/wiki/wikipedia', 266),\n",
       " ('marcolfuck', 260),\n",
       " ('boymamas', 258),\n",
       " (':the', 249),\n",
       " ('penis/////small', 249),\n",
       " ('class=', 248),\n",
       " ('align=', 247),\n",
       " ('|rowspan=', 239),\n",
       " ('edgar181', 238),\n",
       " ('misterwiki', 237),\n",
       " ('valign=', 232),\n",
       " ('به', 232),\n",
       " ('//www.youtube.com/watch', 230),\n",
       " ('tommy2010', 228),\n",
       " ('kfuckity', 227),\n",
       " ('securityfuck', 227),\n",
       " ('supertr0ll', 223),\n",
       " (':you', 220),\n",
       " ('//en.wikipedia.org/wiki/user_talk', 220),\n",
       " ('youbollocks', 217),\n",
       " ('bastered==bastered', 217),\n",
       " ('bgcolor=', 216),\n",
       " ('fart.china', 216),\n",
       " ('از', 214),\n",
       " ('concernthanks', 212),\n",
       " ('wp:3rr', 208),\n",
       " ('ancestryfuck-off-jewish', 207),\n",
       " ('sitush', 204),\n",
       " ('fvckers', 200),\n",
       " ('bbb23', 199),\n",
       " ('༽つameno༼', 199),\n",
       " ('༽つ༼', 199),\n",
       " ('kezami_123', 199),\n",
       " ('apparition11', 192),\n",
       " ('من', 191),\n",
       " ('j.delanoy', 191),\n",
       " ('created/took', 189),\n",
       " ('huura', 188),\n",
       " ('hyehuh', 188),\n",
       " ('deleted.this', 185),\n",
       " ('**', 184),\n",
       " ('padding:0', 182),\n",
       " ('vtsand', 180),\n",
       " ('fan-1967', 180),\n",
       " ('centraliststupid', 179),\n",
       " ('nhrhs2010', 177),\n",
       " ('hornyhorny', 174),\n",
       " ('helloe', 171),\n",
       " ('←', 169),\n",
       " ('*i', 165),\n",
       " ('border:0', 159),\n",
       " ('fdffe7', 158),\n",
       " ('084080', 158),\n",
       " ('sucksgeorge', 158),\n",
       " ('hufgi', 156),\n",
       " ('allumungi', 156),\n",
       " ('adsydfiusagjfasfsduyaidfasgiudf', 154),\n",
       " ('neiln', 153),\n",
       " ('faggt', 152),\n",
       " ('//en.wikipedia.org/wiki/talk', 151),\n",
       " ('☎', 148),\n",
       " ('bleachanhero', 148),\n",
       " (':thanks', 146),\n",
       " ('aidsaids', 146),\n",
       " ('go0verment', 146),\n",
       " ('fatuorum', 144),\n",
       " ('guidelines.', 143),\n",
       " ('=en', 142),\n",
       " ('*tutorial', 141),\n",
       " ('*manual', 140),\n",
       " ('daedalus969', 139),\n",
       " ('fool.what', 139),\n",
       " ('butt-sluts', 139),\n",
       " ('bitchmattythewhite', 139),\n",
       " (':fuck', 135),\n",
       " ('noticeboard/incidents', 134),\n",
       " ('—the', 134),\n",
       " (':it', 132),\n",
       " ('.i', 130),\n",
       " ('lolzblack', 130),\n",
       " ('=1', 129),\n",
       " ('***', 129),\n",
       " (\"'u\", 128),\n",
       " ('haahhahahah', 128),\n",
       " ('fagsgod', 127),\n",
       " ('این', 126),\n",
       " ('permissions-en', 126),\n",
       " ('cheesei', 126),\n",
       " ('nigggers', 124),\n",
       " ('rabbitowenx', 124),\n",
       " ('//en.wikipedia.org/wiki/user', 123),\n",
       " ('gentlty', 122),\n",
       " ('talk|contribs', 121),\n",
       " ('//books.google.com/books', 121),\n",
       " ('=====', 121),\n",
       " ('sambo-queen', 121),\n",
       " ('penis/cockuser', 120),\n",
       " ('♥', 119),\n",
       " ('|image=', 119),\n",
       " ('8===d~~penis', 118),\n",
       " ('†', 116),\n",
       " ('♦', 116),\n",
       " ('है', 116),\n",
       " ('==because', 116),\n",
       " (':if', 114),\n",
       " ('wikipedia_talk', 113),\n",
       " ('✉', 113),\n",
       " ('با', 113),\n",
       " ('►', 112),\n",
       " ('fair_use', 111),\n",
       " ('|listas', 110),\n",
       " ('wikiquette', 110),\n",
       " (':no', 109),\n",
       " ('drmies', 108),\n",
       " (\"'i\", 107),\n",
       " (':that', 107),\n",
       " ('*if', 106),\n",
       " (':*', 106),\n",
       " ('pennnis', 105),\n",
       " ('pensnsnniensnsn', 105),\n",
       " ('therefuck', 104),\n",
       " ('nl33ers', 102),\n",
       " ('ryulong', 101),\n",
       " ('itsuck', 101)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notInVocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx2word = {count:i for count, i in enumerate(embeddings.keys())}\n",
    "word2idx = {idx2word[i]: i for i in idx2word.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1674\n",
      "211860\n",
      "could\n"
     ]
    }
   ],
   "source": [
    "print(word2idx['testing'])\n",
    "print(word2idx['<UNK>'])\n",
    "print(idx2word[123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_factors = 300\n",
    "vocab_size = len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zero_vector_idx = vocab_size #place blank character last\n",
    "idx2word[zero_vector_idx] = ''\n",
    "word2idx[''] = zero_vector_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_emb():\n",
    "    emb = np.zeros((vocab_size+1,n_factors), dtype = 'float32')\n",
    "    for i in range(0, vocab_size):\n",
    "        word = idx2word[i]\n",
    "        emb[i,:] = embeddings[word] #each row is a word\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211862, 300)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parse train/test sets\n",
    "table = str.maketrans(\"\",\"\", string.punctuation)\n",
    "def toksToInds(listToks):\n",
    "    ans = []\n",
    "    for count, i in enumerate(listToks):\n",
    "        try:\n",
    "            ans.append(word2idx[i])\n",
    "        except KeyError:\n",
    "            temp = i.translate(table)\n",
    "            if temp and temp in word2idx.keys():\n",
    "                ans.append(word2idx[temp])\n",
    "            else:\n",
    "                ans.append(word2idx['<UNK>'])        \n",
    "    return np.array(ans)\n",
    "\n",
    "train['idxInput'] = train['tokens'].apply(toksToInds)\n",
    "test['idxInput'] = test['tokens'].apply(toksToInds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mat = sequence.pad_sequences(train['idxInput'].values, maxlen = max_length, value = zero_vector_idx)\n",
    "test_mat = sequence.pad_sequences(test['idxInput'].values, maxlen = max_length, value = zero_vector_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(207442, 500)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                                                                                                                                                                                                                                                                                                                                                                      `` <UNK> more help with a specific editor <UNK> will not stop harassing me on north carolina tar heels related articles . he continually edits out any changes and uses poor reasoning to do so such as `` '' lack of citations '' '' regarding almost any statement . he baits people into <UNK> and carries an extremely poor tone and lack of civility towards his fellow editors . he has had several admin interventions for his behavior , yet wo n't stop . i can not interact with him , and have requested in the past we work things out amicably . his most recent target of contention is the north carolina tar heels men 's basketball page . check out his edits as well as his talk page and you 'll see what i am talking about . thanks. ``\""
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[i] for i in train_mat[0,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "indexTrain = np.random.choice(range(train_mat.shape[0]), size = int(0.9*train_mat.shape[0]), replace = False)\n",
    "indexVal = list(set(range(train_mat.shape[0])) - set(indexTrain))\n",
    "dataInputTrain = train_mat[indexTrain]\n",
    "dataInputVal = train_mat[indexVal]\n",
    "traindf = train.loc[indexTrain]\n",
    "valdf = train.loc[indexVal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18379</td>\n",
       "      <td>20545</td>\n",
       "      <td>19432</td>\n",
       "      <td>20695</td>\n",
       "      <td>19619</td>\n",
       "      <td>20572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2366</td>\n",
       "      <td>200</td>\n",
       "      <td>1313</td>\n",
       "      <td>50</td>\n",
       "      <td>1126</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0  18379         20545    19432   20695   19619          20572\n",
       "1   2366           200     1313      50    1126            173"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valdf[output_names].apply(lambda x: pd.value_counts(np.round(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple starter\n",
    "# vec_input = Input(shape = (max_length,))\n",
    "# x = Embedding(vocab_size+1, n_factors, input_length=max_length,weights = [emb], trainable = False)(vec_input)\n",
    "# x = Conv1D(64,7,activation = 'relu', padding = 'same')(x)\n",
    "# x = MaxPooling1D(2)(x)\n",
    "# # x = Dropout(0.2)(x)\n",
    "\n",
    "# x = Conv1D(64,7,activation = 'relu', padding = 'same')(x)\n",
    "# x = GlobalMaxPooling1D()(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "\n",
    "# x = Dense(32, activation = 'relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "# x = Dense(len(output_names), activation = 'sigmoid')(x)\n",
    "\n",
    "# model = Model(inputs = vec_input, outputs = x)\n",
    "# model.compile(loss='binary_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_input = Input(shape = (max_length,))\n",
    "x = Embedding(vocab_size+1, n_factors, input_length=max_length,weights = [emb], trainable = False)(vec_input)\n",
    "x = BatchNormalization(axis = -1)(x)\n",
    "#conv 1\n",
    "x = Conv1D(32, 7, activation = 'relu', padding = 'same')(x)\n",
    "x = BatchNormalization(axis = -1)(x)\n",
    "x = MaxPooling1D(pool_size = 2)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "#conv 2\n",
    "x = Conv1D(64, 7, activation = 'relu', padding = 'same')(x)\n",
    "x = BatchNormalization(axis = -1)(x)\n",
    "x = MaxPooling1D(pool_size = 2)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "#conv 3\n",
    "x = Conv1D(64, 7, activation = 'relu', padding = 'same')(x)\n",
    "x = BatchNormalization(axis = -1)(x)\n",
    "x = MaxPooling1D(pool_size = 2)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "#conv 4\n",
    "x = Conv1D(128, 7, activation = 'relu', padding = 'same')(x)\n",
    "x = BatchNormalization(axis = -1)(x)\n",
    "x = MaxPooling1D(pool_size = 2)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# #conv 5\n",
    "# x = Conv1D(128, 7, activation = 'relu', padding = 'same')(x)\n",
    "# x = BatchNormalization(axis = -1)(x)\n",
    "# x = MaxPooling1D(pool_size = 2)(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "\n",
    "#dense 1\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(64, activation = 'relu')(x)\n",
    "x = BatchNormalization(axis = -1)(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "#dense 2\n",
    "x = Dense(128, activation = 'relu')(x)\n",
    "x = BatchNormalization(axis = -1)(x)\n",
    "x = Dropout(0.4)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add output layers\n",
    "outputs = []\n",
    "for i in output_names:\n",
    "    intermediate_act = Dense(units = 128, activation = 'relu')(x)\n",
    "    intermediate_act = BatchNormalization(axis = -1)(intermediate_act)\n",
    "    if i in ['toxic','obscene']:\n",
    "        intermediate_act = Dense(units = 512, activation = 'relu')(intermediate_act)\n",
    "        intermediate_act = BatchNormalization(axis = -1)(intermediate_act)\n",
    "    outputs.append(Dense(units = 1, activation = 'sigmoid', name = i)(intermediate_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs = vec_input, outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 500, 300)     63558600    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 500, 300)     1200        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 500, 32)      67232       batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 500, 32)      128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 250, 32)      0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 250, 32)      0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 250, 64)      14400       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 250, 64)      256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 125, 64)      0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 125, 64)      0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 125, 64)      28736       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 125, 64)      256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 62, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 62, 64)       0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 62, 128)      57472       dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 62, 128)      512         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 31, 128)      0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 31, 128)      0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 64)           8256        global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 64)           256         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 64)           0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 128)          8320        dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 128)          512         dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 128)          16512       batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 128)          16512       batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 128)          512         dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 128)          512         dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 256)          33024       batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 128)          16512       batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 256)          33024       batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 128)          16512       batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 128)          16512       batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 128)          16512       batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 256)          1024        dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 128)          512         dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 256)          1024        dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 128)          512         dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 128)          512         dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 128)          512         dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "toxic (Dense)                   (None, 1)            257         batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "severe_toxic (Dense)            (None, 1)            129         batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "obscene (Dense)                 (None, 1)            257         batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "threat (Dense)                  (None, 1)            129         batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "insult (Dense)                  (None, 1)            129         batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "identity_hate (Dense)           (None, 1)            129         batch_normalization_30[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 63,917,406\n",
      "Trainable params: 354,686\n",
      "Non-trainable params: 63,562,720\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(1e-5), loss = 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('weights/cnn_mdl', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, epsilon=1e-4, mode='min')\n",
    "tensor_board = TensorBoard(log_dir='./logs/run1', write_graph = False,)\n",
    "roc_callback = ROCCallBack(training_data = [dataInputTrain, [traindf[i] for i in output_names]],\n",
    "                          validation_data = [dataInputVal, [valdf[i] for i in output_names]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 186697 samples, validate on 20745 samples\n",
      "Epoch 1/200\n",
      "roc-auc: 0.9765 - roc-auc_val: 0.974                                                                                                     0.0656 - threat_loss: 0.0142 - insult_loss: 0.0759 - identity_hate_loss: 0.0307\n",
      "Val ROCs: toxic: 0.9783    severe_toxic: 0.9888    obscene: 0.9873    threat: 0.9537    insult: 0.9812    identity_hate: 0.955\n",
      "186697/186697 [==============================] - 307s 2ms/step - loss: 0.3201 - toxic_loss: 0.1090 - severe_toxic_loss: 0.0248 - obscene_loss: 0.0656 - threat_loss: 0.0141 - insult_loss: 0.0759 - identity_hate_loss: 0.0307 - val_loss: 0.3650 - val_toxic_loss: 0.1231 - val_severe_toxic_loss: 0.0257 - val_obscene_loss: 0.0864 - val_threat_loss: 0.0121 - val_insult_loss: 0.0867 - val_identity_hate_loss: 0.0311\n",
      "Epoch 2/200\n",
      "roc-auc: 0.9792 - roc-auc_val: 0.974                                                                                                     0.0635 - threat_loss: 0.0137 - insult_loss: 0.0736 - identity_hate_loss: 0.0302\n",
      "Val ROCs: toxic: 0.9803    severe_toxic: 0.9839    obscene: 0.9884    threat: 0.9553    insult: 0.9814    identity_hate: 0.9549\n",
      "186697/186697 [==============================] - 304s 2ms/step - loss: 0.3109 - toxic_loss: 0.1055 - severe_toxic_loss: 0.0244 - obscene_loss: 0.0635 - threat_loss: 0.0137 - insult_loss: 0.0736 - identity_hate_loss: 0.0302 - val_loss: 0.3203 - val_toxic_loss: 0.1060 - val_severe_toxic_loss: 0.0249 - val_obscene_loss: 0.0689 - val_threat_loss: 0.0133 - val_insult_loss: 0.0762 - val_identity_hate_loss: 0.0311\n",
      "Epoch 3/200\n",
      "roc-auc: 0.9798 - roc-auc_val: 0.9694                                                                                                    0.0600 - threat_loss: 0.0133 - insult_loss: 0.0709 - identity_hate_loss: 0.0296\n",
      "Val ROCs: toxic: 0.9789    severe_toxic: 0.985    obscene: 0.9884    threat: 0.9257    insult: 0.9824    identity_hate: 0.956\n",
      "186697/186697 [==============================] - 304s 2ms/step - loss: 0.2991 - toxic_loss: 0.1017 - severe_toxic_loss: 0.0237 - obscene_loss: 0.0600 - threat_loss: 0.0133 - insult_loss: 0.0709 - identity_hate_loss: 0.0296 - val_loss: 0.3508 - val_toxic_loss: 0.1128 - val_severe_toxic_loss: 0.0286 - val_obscene_loss: 0.0850 - val_threat_loss: 0.0129 - val_insult_loss: 0.0804 - val_identity_hate_loss: 0.0312\n",
      "Epoch 4/200\n",
      "roc-auc: 0.983 - roc-auc_val: 0.9763                                                                                                     0.0582 - threat_loss: 0.0131 - insult_loss: 0.0697 - identity_hate_loss: 0.0289\n",
      "Val ROCs: toxic: 0.9792    severe_toxic: 0.9887    obscene: 0.989    threat: 0.9632    insult: 0.9823    identity_hate: 0.9553\n",
      "186697/186697 [==============================] - 305s 2ms/step - loss: 0.2892 - toxic_loss: 0.0958 - severe_toxic_loss: 0.0235 - obscene_loss: 0.0582 - threat_loss: 0.0131 - insult_loss: 0.0697 - identity_hate_loss: 0.0289 - val_loss: 0.3249 - val_toxic_loss: 0.1082 - val_severe_toxic_loss: 0.0283 - val_obscene_loss: 0.0686 - val_threat_loss: 0.0122 - val_insult_loss: 0.0766 - val_identity_hate_loss: 0.0310\n",
      "Epoch 5/200\n",
      "roc-auc: 0.9851 - roc-auc_val: 0.98                                                                                                    : 0.0554 - threat_loss: 0.0128 - insult_loss: 0.0664 - identity_hate_loss: 0.0281\n",
      "Val ROCs: toxic: 0.9826    severe_toxic: 0.9894    obscene: 0.9898    threat: 0.9723    insult: 0.9841    identity_hate: 0.9618\n",
      "186697/186697 [==============================] - 305s 2ms/step - loss: 0.2783 - toxic_loss: 0.0924 - severe_toxic_loss: 0.0232 - obscene_loss: 0.0554 - threat_loss: 0.0128 - insult_loss: 0.0664 - identity_hate_loss: 0.0281 - val_loss: 0.3166 - val_toxic_loss: 0.1009 - val_severe_toxic_loss: 0.0242 - val_obscene_loss: 0.0731 - val_threat_loss: 0.0109 - val_insult_loss: 0.0770 - val_identity_hate_loss: 0.0304\n",
      "Epoch 6/200\n",
      "roc-auc: 0.9852 - roc-auc_val: 0.9778                                                                                                    0.0534 - threat_loss: 0.0124 - insult_loss: 0.0653 - identity_hate_loss: 0.0279\n",
      "Val ROCs: toxic: 0.9812    severe_toxic: 0.9894    obscene: 0.9892    threat: 0.9671    insult: 0.9813    identity_hate: 0.9589\n",
      "186697/186697 [==============================] - 304s 2ms/step - loss: 0.2716 - toxic_loss: 0.0894 - severe_toxic_loss: 0.0232 - obscene_loss: 0.0534 - threat_loss: 0.0124 - insult_loss: 0.0653 - identity_hate_loss: 0.0279 - val_loss: 0.3450 - val_toxic_loss: 0.1135 - val_severe_toxic_loss: 0.0279 - val_obscene_loss: 0.0830 - val_threat_loss: 0.0111 - val_insult_loss: 0.0798 - val_identity_hate_loss: 0.0297\n",
      "Epoch 7/200\n",
      "roc-auc: 0.9845 - roc-auc_val: 0.9767                                                                                                    0.0527 - threat_loss: 0.0122 - insult_loss: 0.0641 - identity_hate_loss: 0.0272\n",
      "Val ROCs: toxic: 0.9799    severe_toxic: 0.9893    obscene: 0.9888    threat: 0.9604    insult: 0.9829    identity_hate: 0.9592\n",
      "186697/186697 [==============================] - 304s 2ms/step - loss: 0.2650 - toxic_loss: 0.0859 - severe_toxic_loss: 0.0229 - obscene_loss: 0.0527 - threat_loss: 0.0122 - insult_loss: 0.0641 - identity_hate_loss: 0.0272 - val_loss: 0.3344 - val_toxic_loss: 0.1094 - val_severe_toxic_loss: 0.0256 - val_obscene_loss: 0.0810 - val_threat_loss: 0.0126 - val_insult_loss: 0.0743 - val_identity_hate_loss: 0.0315\n",
      "Epoch 8/200\n",
      "roc-auc: 0.9874 - roc-auc_val: 0.9799                                                                                                    0.0510 - threat_loss: 0.0121 - insult_loss: 0.0625 - identity_hate_loss: 0.0270\n",
      "Val ROCs: toxic: 0.9808    severe_toxic: 0.9899    obscene: 0.9901    threat: 0.9739    insult: 0.9842    identity_hate: 0.9605\n",
      "186697/186697 [==============================] - 304s 2ms/step - loss: 0.2589 - toxic_loss: 0.0838 - severe_toxic_loss: 0.0225 - obscene_loss: 0.0510 - threat_loss: 0.0121 - insult_loss: 0.0625 - identity_hate_loss: 0.0270 - val_loss: 0.3285 - val_toxic_loss: 0.1080 - val_severe_toxic_loss: 0.0250 - val_obscene_loss: 0.0808 - val_threat_loss: 0.0106 - val_insult_loss: 0.0738 - val_identity_hate_loss: 0.0304\n",
      "Epoch 9/200\n",
      "roc-auc: 0.988 - roc-auc_val: 0.9782                                                                                                     0.0494 - threat_loss: 0.0117 - insult_loss: 0.0612 - identity_hate_loss: 0.0267\n",
      "Val ROCs: toxic: 0.9791    severe_toxic: 0.9894    obscene: 0.9888    threat: 0.9658    insult: 0.9829    identity_hate: 0.9632\n",
      "186697/186697 [==============================] - 311s 2ms/step - loss: 0.2520 - toxic_loss: 0.0807 - severe_toxic_loss: 0.0223 - obscene_loss: 0.0494 - threat_loss: 0.0117 - insult_loss: 0.0612 - identity_hate_loss: 0.0267 - val_loss: 0.3259 - val_toxic_loss: 0.1120 - val_severe_toxic_loss: 0.0272 - val_obscene_loss: 0.0730 - val_threat_loss: 0.0116 - val_insult_loss: 0.0730 - val_identity_hate_loss: 0.0291\n",
      "Epoch 10/200\n",
      "roc-auc: 0.9882 - roc-auc_val: 0.9746                                                                                                    0.0481 - threat_loss: 0.0117 - insult_loss: 0.0603 - identity_hate_loss: 0.0263\n",
      "Val ROCs: toxic: 0.9731    severe_toxic: 0.9894    obscene: 0.9883    threat: 0.9573    insult: 0.9814    identity_hate: 0.9584\n",
      "186697/186697 [==============================] - 306s 2ms/step - loss: 0.2465 - toxic_loss: 0.0780 - severe_toxic_loss: 0.0221 - obscene_loss: 0.0481 - threat_loss: 0.0117 - insult_loss: 0.0603 - identity_hate_loss: 0.0263 - val_loss: 0.4043 - val_toxic_loss: 0.1429 - val_severe_toxic_loss: 0.0246 - val_obscene_loss: 0.1082 - val_threat_loss: 0.0110 - val_insult_loss: 0.0870 - val_identity_hate_loss: 0.0306\n",
      "Epoch 11/200\n",
      "186688/186697 [============================>.] - ETA: 0s - loss: 0.2431 - toxic_loss: 0.0772 - severe_toxic_loss: 0.0224 - obscene_loss: 0.0476 - threat_loss: 0.0113 - insult_loss: 0.0588 - identity_hate_loss: 0.0259\n",
      "Epoch 00011: reducing learning rate to 0.00010000000474974513.\n",
      "roc-auc: 0.9894 - roc-auc_val: 0.9802                                                                                                    \n",
      "Val ROCs: toxic: 0.9788    severe_toxic: 0.9898    obscene: 0.9887    threat: 0.9744    insult: 0.9833    identity_hate: 0.9664\n",
      "186697/186697 [==============================] - 303s 2ms/step - loss: 0.2431 - toxic_loss: 0.0772 - severe_toxic_loss: 0.0224 - obscene_loss: 0.0476 - threat_loss: 0.0113 - insult_loss: 0.0588 - identity_hate_loss: 0.0259 - val_loss: 0.3467 - val_toxic_loss: 0.1166 - val_severe_toxic_loss: 0.0309 - val_obscene_loss: 0.0834 - val_threat_loss: 0.0106 - val_insult_loss: 0.0765 - val_identity_hate_loss: 0.0286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/200\n",
      "roc-auc: 0.9899 - roc-auc_val: 0.9764                                                                                                    0.0423 - threat_loss: 0.0107 - insult_loss: 0.0545 - identity_hate_loss: 0.0249\n",
      "Val ROCs: toxic: 0.9759    severe_toxic: 0.9896    obscene: 0.9884    threat: 0.9586    insult: 0.9834    identity_hate: 0.9623\n",
      "186697/186697 [==============================] - 301s 2ms/step - loss: 0.2209 - toxic_loss: 0.0673 - severe_toxic_loss: 0.0212 - obscene_loss: 0.0423 - threat_loss: 0.0107 - insult_loss: 0.0545 - identity_hate_loss: 0.0249 - val_loss: 0.3512 - val_toxic_loss: 0.1243 - val_severe_toxic_loss: 0.0284 - val_obscene_loss: 0.0821 - val_threat_loss: 0.0107 - val_insult_loss: 0.0767 - val_identity_hate_loss: 0.0291\n",
      "Epoch 13/200\n",
      "roc-auc: 0.9899 - roc-auc_val: 0.9754                                                                                                    0.0410 - threat_loss: 0.0105 - insult_loss: 0.0535 - identity_hate_loss: 0.0244\n",
      "Val ROCs: toxic: 0.9744    severe_toxic: 0.9896    obscene: 0.988    threat: 0.9585    insult: 0.9835    identity_hate: 0.9584\n",
      "186697/186697 [==============================] - 301s 2ms/step - loss: 0.2152 - toxic_loss: 0.0647 - severe_toxic_loss: 0.0210 - obscene_loss: 0.0410 - threat_loss: 0.0105 - insult_loss: 0.0535 - identity_hate_loss: 0.0244 - val_loss: 0.3588 - val_toxic_loss: 0.1316 - val_severe_toxic_loss: 0.0276 - val_obscene_loss: 0.0814 - val_threat_loss: 0.0108 - val_insult_loss: 0.0778 - val_identity_hate_loss: 0.0296\n",
      "Epoch 14/200\n",
      "roc-auc: 0.9905 - roc-auc_val: 0.9754                                                                                                    0.0407 - threat_loss: 0.0104 - insult_loss: 0.0527 - identity_hate_loss: 0.0245\n",
      "Val ROCs: toxic: 0.9744    severe_toxic: 0.9896    obscene: 0.9883    threat: 0.9581    insult: 0.9834    identity_hate: 0.9585\n",
      "186697/186697 [==============================] - 301s 2ms/step - loss: 0.2123 - toxic_loss: 0.0631 - severe_toxic_loss: 0.0209 - obscene_loss: 0.0407 - threat_loss: 0.0104 - insult_loss: 0.0527 - identity_hate_loss: 0.0245 - val_loss: 0.3544 - val_toxic_loss: 0.1313 - val_severe_toxic_loss: 0.0270 - val_obscene_loss: 0.0794 - val_threat_loss: 0.0108 - val_insult_loss: 0.0764 - val_identity_hate_loss: 0.0295\n",
      "Epoch 15/200\n",
      "roc-auc: 0.9902 - roc-auc_val: 0.9735                                                                                                    0.0402 - threat_loss: 0.0105 - insult_loss: 0.0520 - identity_hate_loss: 0.0243\n",
      "Val ROCs: toxic: 0.9704    severe_toxic: 0.9896    obscene: 0.9876    threat: 0.9565    insult: 0.9821    identity_hate: 0.9545\n",
      "186697/186697 [==============================] - 301s 2ms/step - loss: 0.2093 - toxic_loss: 0.0616 - severe_toxic_loss: 0.0207 - obscene_loss: 0.0402 - threat_loss: 0.0105 - insult_loss: 0.0520 - identity_hate_loss: 0.0243 - val_loss: 0.3670 - val_toxic_loss: 0.1398 - val_severe_toxic_loss: 0.0284 - val_obscene_loss: 0.0798 - val_threat_loss: 0.0109 - val_insult_loss: 0.0781 - val_identity_hate_loss: 0.0299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc9b5d97438>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_value(model.optimizer.lr, 1e-3)\n",
    "model.fit(x = dataInputTrain, \n",
    "          y = [traindf[i] for i in output_names],\n",
    "         batch_size = 64, epochs = 200, validation_data = [dataInputVal, [valdf[i] for i in output_names]],\n",
    "         callbacks=[earlyStopping, mcp_save, reduce_lr_loss, tensor_board, roc_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining what the model got wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_val = model.predict(dataInputVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for count,i in enumerate(output_names):\n",
    "#     print('---------------' + i + '---------------------')\n",
    "#     dif = (valdf[i] - pred_val[count].flatten()).abs().sort_values(ascending = False)\n",
    "#     most_dif = valdf.loc[dif.index[:2]]\n",
    "#     for count2, j in enumerate(most_dif.iterrows()):\n",
    "#         print('Predicted', pred_val[count].flatten()[valdf[i].index.get_loc(j[0])]\n",
    "#               , 'Actual', j[1][i],\n",
    "#              '\\n',\n",
    "#              j[1]['comment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(test_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for count,i in enumerate(output_names):\n",
    "    test[i] = pred[count].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].to_csv('data/answers/cnn1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_stopped = load_model('weights/cnn_mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_stopped = model_stopped.predict(test_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for count,i in enumerate(output_names):\n",
    "    test[i] = pred[count].flatten()\n",
    "test[['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].to_csv('data/answers/cnnStopped.csv', index = False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
